{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller 4  - Competencia: Clasificación de Peptidos Antimicrobianos\n",
    "\n",
    "Se cuenta con una recopilación de bases de datos de *péptidos* de dos tipos: *antimicrobianos* y *No antimicrobianos*. Para cada conjunto de datos se tienen 1700 descriptores o características fisicoquímicas de los *péptidos*. \n",
    "\n",
    "El objetivo de este Notebook es realizar un proceso de clasificación de los péptidos de ambas clases empleando 3 métodos de clasificación: Regresión Logística, Bosque Aleatorio y Descenso de Gradiente Estocástico. Para ello se dividirá la base de datos en un 80% para entrenamiento y 20% para probar los modelos. \n",
    "\n",
    "El procedimiento de trabajo es el siguiente:\n",
    "1. Se realizará el proceso de clasificación con todas las características, sin modificar los hyperparámetros.\n",
    "2. Se empleará `GridSearch`, un método para ajustar hyperparámetros de manera automática. \n",
    "3. Se seleccionarán los hyperparámetros a utilizar para los siguientes pasos.\n",
    "4. Se aplicará un proceso de eliminación de características recursivo para extraer las características más importantes.\n",
    "5. Se aplicará un método de extracción de características.\n",
    "\n",
    "En cada paso de medirán diferentes métricas para evaluar el desempeño de los clasificadores.\n",
    "\n",
    "\n",
    "Los integrates del grupo son:\n",
    "- Lina Victoria Parra Duque\n",
    "- Janick Alberto Reales Salas\n",
    "- Luisa Fernanda Rios Piedrahita\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se importan algunas librerías requeridas\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a cargar las bases de datos de péptidos a emplear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8322, 1760)\n",
      "(13679, 1760)\n",
      "(1623, 1761)\n"
     ]
    }
   ],
   "source": [
    "## Base de datos positivos (peptidos antimicrobianos)\n",
    "positivos = pd.read_csv(\"DatosPositivos2.csv\", index_col = 0)\n",
    "\n",
    "## Base de datos negativos (peptidos no antimicrobianos)\n",
    "negativos = pd.read_csv(\"DatosNegativos2.csv\")\n",
    "\n",
    "## Conjunto de validación\n",
    "validacion = pd.read_csv(\"DatosValidacion2.csv\", index_col = 0)\n",
    "\n",
    "print(positivos.shape)\n",
    "print(negativos.shape)\n",
    "print(validacion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sequence', 'length', 'molecular_weight', 'charge', 'charge_density',\n",
      "       'isoelectric_point', 'gravy', 'instability_index', 'aromaticity',\n",
      "       'aliphatic_index',\n",
      "       ...\n",
      "       'embed_2_91', 'embed_2_92', 'embed_2_93', 'embed_2_94', 'embed_2_95',\n",
      "       'embed_2_96', 'embed_2_97', 'embed_2_98', 'embed_2_99', 'class'],\n",
      "      dtype='object', length=1760)\n",
      "Index(['sequence', 'length', 'molecular_weight', 'charge', 'charge_density',\n",
      "       'isoelectric_point', 'gravy', 'instability_index', 'aromaticity',\n",
      "       'aliphatic_index',\n",
      "       ...\n",
      "       'embed_2_91', 'embed_2_92', 'embed_2_93', 'embed_2_94', 'embed_2_95',\n",
      "       'embed_2_96', 'embed_2_97', 'embed_2_98', 'embed_2_99', 'class'],\n",
      "      dtype='object', length=1760)\n",
      "Index(['Unnamed: 0.1', 'sequence', 'length', 'molecular_weight', 'charge',\n",
      "       'charge_density', 'isoelectric_point', 'gravy', 'instability_index',\n",
      "       'aromaticity',\n",
      "       ...\n",
      "       'embed_2_91', 'embed_2_92', 'embed_2_93', 'embed_2_94', 'embed_2_95',\n",
      "       'embed_2_96', 'embed_2_97', 'embed_2_98', 'embed_2_99', 'class'],\n",
      "      dtype='object', length=1761)\n"
     ]
    }
   ],
   "source": [
    "## Verificamos las características contenidas en cada base de datos\n",
    "print(positivos.columns)\n",
    "print(negativos.columns)\n",
    "print(validacion.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de datos de validación contiene una columba extra (Unnamed: 0.1) con los índices, por lo que se procede a eliminarla para que los tres conjuntos de datos cuenten con el mismo número de columnas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1623, 1760)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>length</th>\n",
       "      <th>molecular_weight</th>\n",
       "      <th>charge</th>\n",
       "      <th>charge_density</th>\n",
       "      <th>isoelectric_point</th>\n",
       "      <th>gravy</th>\n",
       "      <th>instability_index</th>\n",
       "      <th>aromaticity</th>\n",
       "      <th>aliphatic_index</th>\n",
       "      <th>...</th>\n",
       "      <th>embed_2_91</th>\n",
       "      <th>embed_2_92</th>\n",
       "      <th>embed_2_93</th>\n",
       "      <th>embed_2_94</th>\n",
       "      <th>embed_2_95</th>\n",
       "      <th>embed_2_96</th>\n",
       "      <th>embed_2_97</th>\n",
       "      <th>embed_2_98</th>\n",
       "      <th>embed_2_99</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>VVCACRRALCLPRERRAGFCRIRGRIHPLCCRR</td>\n",
       "      <td>33</td>\n",
       "      <td>3897.77</td>\n",
       "      <td>8.691</td>\n",
       "      <td>0.002230</td>\n",
       "      <td>11.404358</td>\n",
       "      <td>-0.112121</td>\n",
       "      <td>84.766667</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>85.757576</td>\n",
       "      <td>...</td>\n",
       "      <td>2.771416</td>\n",
       "      <td>-0.851930</td>\n",
       "      <td>-0.459909</td>\n",
       "      <td>0.909622</td>\n",
       "      <td>1.402783</td>\n",
       "      <td>-3.848056</td>\n",
       "      <td>-0.528822</td>\n",
       "      <td>-0.740751</td>\n",
       "      <td>-0.257650</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>GICACRRRFCPNSERFSGYCRVNGARYVRCCSRR</td>\n",
       "      <td>34</td>\n",
       "      <td>4003.64</td>\n",
       "      <td>7.590</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>10.196106</td>\n",
       "      <td>-0.638235</td>\n",
       "      <td>76.035294</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>34.411765</td>\n",
       "      <td>...</td>\n",
       "      <td>2.356963</td>\n",
       "      <td>-0.590644</td>\n",
       "      <td>-0.433246</td>\n",
       "      <td>0.362768</td>\n",
       "      <td>1.204798</td>\n",
       "      <td>-3.838024</td>\n",
       "      <td>-0.859893</td>\n",
       "      <td>-1.086264</td>\n",
       "      <td>0.052278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>LRDLVCYCRSRGCKGRERMNGTCRKGHLLYTLCCR</td>\n",
       "      <td>35</td>\n",
       "      <td>4121.92</td>\n",
       "      <td>6.689</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>9.611023</td>\n",
       "      <td>-0.551429</td>\n",
       "      <td>16.851429</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.762231</td>\n",
       "      <td>-1.182640</td>\n",
       "      <td>-0.908285</td>\n",
       "      <td>0.802487</td>\n",
       "      <td>1.546229</td>\n",
       "      <td>-4.543212</td>\n",
       "      <td>-0.786475</td>\n",
       "      <td>-0.477468</td>\n",
       "      <td>-0.178319</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>RRCICTTRTCRFPYRRLGTCIFQNRVYTFCC</td>\n",
       "      <td>31</td>\n",
       "      <td>3838.56</td>\n",
       "      <td>6.589</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>9.802917</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>53.977419</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>47.096774</td>\n",
       "      <td>...</td>\n",
       "      <td>1.742604</td>\n",
       "      <td>-0.981736</td>\n",
       "      <td>-0.098695</td>\n",
       "      <td>0.614634</td>\n",
       "      <td>0.774174</td>\n",
       "      <td>-2.818301</td>\n",
       "      <td>-0.395545</td>\n",
       "      <td>-0.738405</td>\n",
       "      <td>0.369630</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>VCSCRLVFCRRTELRVGNCLIGGVSFTYCCTRV</td>\n",
       "      <td>33</td>\n",
       "      <td>3715.45</td>\n",
       "      <td>3.591</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>8.981384</td>\n",
       "      <td>0.660606</td>\n",
       "      <td>62.042424</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>91.212121</td>\n",
       "      <td>...</td>\n",
       "      <td>1.734715</td>\n",
       "      <td>-0.830263</td>\n",
       "      <td>-0.386610</td>\n",
       "      <td>0.836167</td>\n",
       "      <td>0.859871</td>\n",
       "      <td>-3.665342</td>\n",
       "      <td>-0.922017</td>\n",
       "      <td>-0.364447</td>\n",
       "      <td>-0.055827</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1760 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              sequence  length  molecular_weight  charge  \\\n",
       "0    VVCACRRALCLPRERRAGFCRIRGRIHPLCCRR      33           3897.77   8.691   \n",
       "1   GICACRRRFCPNSERFSGYCRVNGARYVRCCSRR      34           4003.64   7.590   \n",
       "2  LRDLVCYCRSRGCKGRERMNGTCRKGHLLYTLCCR      35           4121.92   6.689   \n",
       "3      RRCICTTRTCRFPYRRLGTCIFQNRVYTFCC      31           3838.56   6.589   \n",
       "4    VCSCRLVFCRRTELRVGNCLIGGVSFTYCCTRV      33           3715.45   3.591   \n",
       "\n",
       "   charge_density  isoelectric_point     gravy  instability_index  \\\n",
       "0        0.002230          11.404358 -0.112121          84.766667   \n",
       "1        0.001896          10.196106 -0.638235          76.035294   \n",
       "2        0.001623           9.611023 -0.551429          16.851429   \n",
       "3        0.001717           9.802917 -0.200000          53.977419   \n",
       "4        0.000967           8.981384  0.660606          62.042424   \n",
       "\n",
       "   aromaticity  aliphatic_index  ...  embed_2_91  embed_2_92  embed_2_93  \\\n",
       "0     0.030303        85.757576  ...    2.771416   -0.851930   -0.459909   \n",
       "1     0.117647        34.411765  ...    2.356963   -0.590644   -0.433246   \n",
       "2     0.057143        64.000000  ...    2.762231   -1.182640   -0.908285   \n",
       "3     0.161290        47.096774  ...    1.742604   -0.981736   -0.098695   \n",
       "4     0.090909        91.212121  ...    1.734715   -0.830263   -0.386610   \n",
       "\n",
       "   embed_2_94  embed_2_95  embed_2_96  embed_2_97  embed_2_98  embed_2_99  \\\n",
       "0    0.909622    1.402783   -3.848056   -0.528822   -0.740751   -0.257650   \n",
       "1    0.362768    1.204798   -3.838024   -0.859893   -1.086264    0.052278   \n",
       "2    0.802487    1.546229   -4.543212   -0.786475   -0.477468   -0.178319   \n",
       "3    0.614634    0.774174   -2.818301   -0.395545   -0.738405    0.369630   \n",
       "4    0.836167    0.859871   -3.665342   -0.922017   -0.364447   -0.055827   \n",
       "\n",
       "   class  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  \n",
       "\n",
       "[5 rows x 1760 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Se elimina columna con índice de datos de validación\n",
    "\n",
    "validacion = validacion.iloc[:, 1:]\n",
    "print(validacion.shape)\n",
    "validacion.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo siguiente a hacer es unir las bases de datos en una sola llamada \"*database*\", para posteriormente separar la primera columna, la cual contiene los nombres de cada secuencia de péptidos y no es relevate para el proceso de clasificación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se concatenan las bases\n",
    "\n",
    "database = pd.concat([positivos, negativos, validacion], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23624, 1759)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# se elimina columna de texto:\n",
    "\n",
    "sequence = database['sequence']\n",
    "database = database.iloc[:, 1:]\n",
    "database.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miremos cómo se encuentra distribuídas las clases de péptidos en la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    15033\n",
       "1     8591\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Distribución de las clases de peptidos contenidas en la base de datos.\n",
    "\n",
    "database['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(x='class', data=database, palette='hls')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestra base de datos está compuesta en casi un 63% de péptidos no antrimicrobianos (class = 0) y 37% de péptidos microbianos (class = 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora separamos la columna '*class*' del conjunto de características para aplicar un proceso de *normalización* sobre este último. Con esta normalización queremos llevar todas las características a una misma escala, que permita un mejor desempeño de los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se divide la base de datos\n",
    "\n",
    "data = database.drop(['class'], axis=1) # conjunto de carasterísticas\n",
    "columnas = data.columns      # nombres de las columnas/características\n",
    "target = database['class']   # clase de los péptidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se aplica normalización sobre el conjunto de características\n",
    "\n",
    "data_norm = preprocessing.normalize(data, norm='l2', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>molecular_weight</th>\n",
       "      <th>charge</th>\n",
       "      <th>charge_density</th>\n",
       "      <th>isoelectric_point</th>\n",
       "      <th>gravy</th>\n",
       "      <th>instability_index</th>\n",
       "      <th>aromaticity</th>\n",
       "      <th>aliphatic_index</th>\n",
       "      <th>boman_index</th>\n",
       "      <th>...</th>\n",
       "      <th>embed_2_90</th>\n",
       "      <th>embed_2_91</th>\n",
       "      <th>embed_2_92</th>\n",
       "      <th>embed_2_93</th>\n",
       "      <th>embed_2_94</th>\n",
       "      <th>embed_2_95</th>\n",
       "      <th>embed_2_96</th>\n",
       "      <th>embed_2_97</th>\n",
       "      <th>embed_2_98</th>\n",
       "      <th>embed_2_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.006130</td>\n",
       "      <td>0.005887</td>\n",
       "      <td>0.010263</td>\n",
       "      <td>0.010246</td>\n",
       "      <td>0.007871</td>\n",
       "      <td>-0.009469</td>\n",
       "      <td>0.008451</td>\n",
       "      <td>0.006390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009508</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003859</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>0.001525</td>\n",
       "      <td>-0.002609</td>\n",
       "      <td>0.007585</td>\n",
       "      <td>0.003990</td>\n",
       "      <td>-0.005448</td>\n",
       "      <td>-0.000517</td>\n",
       "      <td>-0.008588</td>\n",
       "      <td>0.001712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.005162</td>\n",
       "      <td>0.004815</td>\n",
       "      <td>0.006535</td>\n",
       "      <td>0.007977</td>\n",
       "      <td>0.008063</td>\n",
       "      <td>0.006413</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010008</td>\n",
       "      <td>-0.002086</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001741</td>\n",
       "      <td>0.003078</td>\n",
       "      <td>-0.007457</td>\n",
       "      <td>-0.005970</td>\n",
       "      <td>-0.001380</td>\n",
       "      <td>-0.001512</td>\n",
       "      <td>-0.003999</td>\n",
       "      <td>-0.005291</td>\n",
       "      <td>-0.003582</td>\n",
       "      <td>-0.002984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.005485</td>\n",
       "      <td>0.005955</td>\n",
       "      <td>0.010550</td>\n",
       "      <td>0.010414</td>\n",
       "      <td>0.009411</td>\n",
       "      <td>-0.004293</td>\n",
       "      <td>0.004986</td>\n",
       "      <td>0.009523</td>\n",
       "      <td>0.003307</td>\n",
       "      <td>0.004552</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001257</td>\n",
       "      <td>0.005854</td>\n",
       "      <td>0.005131</td>\n",
       "      <td>-0.001131</td>\n",
       "      <td>0.013715</td>\n",
       "      <td>0.012943</td>\n",
       "      <td>-0.009549</td>\n",
       "      <td>-0.006825</td>\n",
       "      <td>-0.006487</td>\n",
       "      <td>0.005322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.006453</td>\n",
       "      <td>0.006128</td>\n",
       "      <td>0.004216</td>\n",
       "      <td>0.004044</td>\n",
       "      <td>0.008480</td>\n",
       "      <td>0.003143</td>\n",
       "      <td>0.001968</td>\n",
       "      <td>0.004047</td>\n",
       "      <td>0.008327</td>\n",
       "      <td>0.001710</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006106</td>\n",
       "      <td>0.008723</td>\n",
       "      <td>-0.000233</td>\n",
       "      <td>-0.006760</td>\n",
       "      <td>0.011987</td>\n",
       "      <td>0.008103</td>\n",
       "      <td>-0.006340</td>\n",
       "      <td>-0.005677</td>\n",
       "      <td>-0.006845</td>\n",
       "      <td>0.006009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.008066</td>\n",
       "      <td>0.007743</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>0.006037</td>\n",
       "      <td>0.002832</td>\n",
       "      <td>0.003619</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>0.002960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013453</td>\n",
       "      <td>0.007337</td>\n",
       "      <td>-0.016623</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.008107</td>\n",
       "      <td>0.017724</td>\n",
       "      <td>-0.010720</td>\n",
       "      <td>-0.009756</td>\n",
       "      <td>-0.016503</td>\n",
       "      <td>-0.000963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1758 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     length  molecular_weight    charge  charge_density  isoelectric_point  \\\n",
       "0  0.006130          0.005887  0.010263        0.010246           0.007871   \n",
       "1  0.005162          0.004815  0.006535        0.007977           0.008063   \n",
       "2  0.005485          0.005955  0.010550        0.010414           0.009411   \n",
       "3  0.006453          0.006128  0.004216        0.004044           0.008480   \n",
       "4  0.008066          0.007743  0.001172        0.000890           0.006037   \n",
       "\n",
       "      gravy  instability_index  aromaticity  aliphatic_index  boman_index  \\\n",
       "0 -0.009469           0.008451     0.006390         0.000000     0.009508   \n",
       "1  0.006413           0.002421     0.000000         0.010008    -0.002086   \n",
       "2 -0.004293           0.004986     0.009523         0.003307     0.004552   \n",
       "3  0.003143           0.001968     0.004047         0.008327     0.001710   \n",
       "4  0.002832           0.003619     0.003238         0.002220     0.002960   \n",
       "\n",
       "   ...  embed_2_90  embed_2_91  embed_2_92  embed_2_93  embed_2_94  \\\n",
       "0  ...   -0.003859    0.004634    0.001525   -0.002609    0.007585   \n",
       "1  ...   -0.001741    0.003078   -0.007457   -0.005970   -0.001380   \n",
       "2  ...   -0.001257    0.005854    0.005131   -0.001131    0.013715   \n",
       "3  ...   -0.006106    0.008723   -0.000233   -0.006760    0.011987   \n",
       "4  ...   -0.013453    0.007337   -0.016623    0.000291    0.008107   \n",
       "\n",
       "   embed_2_95  embed_2_96  embed_2_97  embed_2_98  embed_2_99  \n",
       "0    0.003990   -0.005448   -0.000517   -0.008588    0.001712  \n",
       "1   -0.001512   -0.003999   -0.005291   -0.003582   -0.002984  \n",
       "2    0.012943   -0.009549   -0.006825   -0.006487    0.005322  \n",
       "3    0.008103   -0.006340   -0.005677   -0.006845    0.006009  \n",
       "4    0.017724   -0.010720   -0.009756   -0.016503   -0.000963  \n",
       "\n",
       "[5 rows x 1758 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se crea un dataframe con el conjunto de características normalizado:\n",
    "\n",
    "data = pd.DataFrame(data_norm, columns=columnas)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, procedemos a dividir el conjunto de datos en datos en un 80% para entrenamiento y 20% para prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora cargamos los modelos que vamos a emplear para la clasificación de los datos, además del método de Validación Cruzada, con el cual prodeceremos a medir el desempeño de los clasificadores en el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Clasificación con todas las características\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## se cargan los modelos\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos los tres modelos con los datos de entrenamiento y empleamos validación cruzada, dividiendo el modelo en 5 partes o folds (cv=5). El método de validación cruzada entrena cada modelo con *k-1* folds y lo valida con el fold restante, midiendo la precisión en cada paso. Los rendimientos que se imprimen a continuación son el promedio obtenido al aplicar validación cruzada (5 folds), con cada modelo, sobre la base de datos de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se obtiene un rendimiento de 84.6% con Regresión Logística\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se obtiene un rendimiento de 87.4% con Random Forest\n",
      "Se obtiene un redimiento de 85.1% con SGD\n"
     ]
    }
   ],
   "source": [
    "# Se ajusta el modelo de Regresión Logística (lr) a los datos y se mide el rendimiento promedio\n",
    "lr = LogisticRegression() \n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "score = cross_val_score(lr, X_train, y_train, cv=5) \n",
    "print(\"Se obtiene un rendimiento de {0:.1%} con Regresión Logística\".format(score.mean()))\n",
    "\n",
    "\n",
    "## Entrenamos el modelo de Bosque Aleatorio y medimos el rendimiento promedio\n",
    "rf = RandomForestClassifier()  \n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "score = cross_val_score(rf, X_train, y_train, cv=5)\n",
    "print(\"Se obtiene un rendimiento de {0:.1%} con Random Forest\".format(score.mean()))\n",
    "\n",
    "\n",
    "## Entrenamos el modelo de Descenso del Gradiente Estocástico y medimos el rendimiento promedio\n",
    "sgd = linear_model.SGDClassifier() \n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "score = cross_val_score(sgd, X_train, y_train, cv=5)\n",
    "print(\"Se obtiene un redimiento de {0:.1%} con SGD\".format(score.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que al entrenar los modelos sin realizar modificaciones a sus hyperparámetros se obtiene un mayor rendimiento en el conjunto de entrenamiento con el modelo de Bosque Aleatorio (87,4%), seguido del de Regresión Logística (84.6%) y por último SGD (85.1%). \n",
    "\n",
    "Utilizamos las métricas precision, recall, F1-score y confusion matrix para medir el desempeño de los clasifidores en el conjunto de prueba. \n",
    "\n",
    "La **matriz de confusión** nos compara las predicciones con los datos reales, donde cada columna de la matriz representa el número de predicciones de cada clase, mientras que las filas representan la clase real. El **precision** nos mide la capacidad del clasificador para no clasificar como positiva una muestra negativa (tp/tp+fp). El **recall** mide la capacidad del clasificador para clasificar las muestras positivas (tp/tp+fn). El **f1-score** es el promedio balaceado entre *precision* y *recall* (2x(precicion x recall)/(precision+recall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se obtiene una precisión de 92.6% con Regresión Logística\n",
      "Se obtiene una precisión de 92.4% con Random Forest\n",
      "Se obtiene una precisión de 93.4% con SGD\n",
      "\n",
      "Se obtiene un recall de 65.0% con Regresión Logística\n",
      "Se obtiene un recall de 73.8% con Random Forest\n",
      "Se obtiene un recall de 65.7% con SGD\n",
      "\n",
      "Se obtiene un F1-score de 76.3% con Regresión Logística\n",
      "Se obtiene un F1-score de 82.1% con Random Forest\n",
      "Se obtiene un F1-score de 77.1% con SGD\n"
     ]
    }
   ],
   "source": [
    "## Evaluación de los clasificadores\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "##Precision\n",
    "lr_score = precision_score(y_test, lr.predict(X_test))\n",
    "rf_score = precision_score(y_test, rf.predict(X_test))\n",
    "sgd_score = precision_score(y_test, sgd.predict(X_test))\n",
    "\n",
    "print(\"Se obtiene una precisión de {0:.1%} con Regresión Logística\".format(lr_score))\n",
    "print(\"Se obtiene una precisión de {0:.1%} con Random Forest\".format(rf_score))\n",
    "print(\"Se obtiene una precisión de {0:.1%} con SGD\\n\".format(sgd_score))\n",
    "\n",
    "\n",
    "##Recall\n",
    "lr_score = recall_score(y_test, lr.predict(X_test))\n",
    "rf_score = recall_score(y_test, rf.predict(X_test))\n",
    "sgd_score = recall_score(y_test, sgd.predict(X_test))\n",
    "\n",
    "print(\"Se obtiene un recall de {0:.1%} con Regresión Logística\".format(lr_score))\n",
    "print(\"Se obtiene un recall de {0:.1%} con Random Forest\".format(rf_score))\n",
    "print(\"Se obtiene un recall de {0:.1%} con SGD\\n\".format(sgd_score))\n",
    "\n",
    "\n",
    "##F1_score\n",
    "lr_score = f1_score(y_test, lr.predict(X_test))\n",
    "rf_score = f1_score(y_test, rf.predict(X_test))\n",
    "sgd_score = f1_score(y_test, sgd.predict(X_test))\n",
    "\n",
    "print(\"Se obtiene un F1-score de {0:.1%} con Regresión Logística\".format(lr_score))\n",
    "print(\"Se obtiene un F1-score de {0:.1%} con Random Forest\".format(rf_score))\n",
    "print(\"Se obtiene un F1-score de {0:.1%} con SGD\".format(sgd_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión Regresión Lineal:\n",
      " [[2903   90]\n",
      " [ 607 1125]] \n",
      "\n",
      "Matriz de confusión Random Forest:\n",
      " [[2888  105]\n",
      " [ 454 1278]] \n",
      "\n",
      "Matriz de confusión SGD:\n",
      " [[2912   81]\n",
      " [ 594 1138]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Confusion matrix\n",
    "lr_cf = confusion_matrix(y_test, lr.predict(X_test))\n",
    "print(\"Matriz de confusión Regresión Lineal:\\n\", lr_cf, \"\\n\")\n",
    "\n",
    "rf_cf = confusion_matrix(y_test, rf.predict(X_test))\n",
    "print(\"Matriz de confusión Random Forest:\\n\", rf_cf, \"\\n\")\n",
    "\n",
    "sgd_cf = confusion_matrix(y_test, sgd.predict(X_test))\n",
    "print(\"Matriz de confusión SGD:\\n\", sgd_cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observaciones:\n",
    "- El modelo de Descenso de Gradiente Estocástico mostró un desempeño ligeramente superior, de acuerdo con la métrica de *Precision*. Sin embargo, su rendimiento es el más bajo, tomando en cuenta los resultados obtenidos para el Recall y el F1-score. Indicando así que este modelo cumple un buen papel a la hora de clasificar a los *péptidos No-antimicrobianos* en su categoría correcta, y que no es tan bueno cuando se trata de clasificar a los *péptidos antimicrobianos*. Se puede constatar esto al mirar su matriz de confusión.\n",
    "- De acuerdo con el *Precision*, los métodos de Regresión Logística y Random Forest presentaron un rendimiento similar (con los hyperparámetros por defecto). Mientras que ambos modelos difieren en las métricas de *recall* y *f1-score*. Indicando que la capacidad del modelo de Regresión logística es similar a la del modelo de Bosque Aleatorio a la hora de no clasificar Péptidos No-Antimicrobianos como Péptidos Antimicrioanos. Sin embargo, el modelo de Bosque aleatorio se comporta mejor para clasificar los péptidos antimicrobianos en su categoría correcta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajustando los Hyperparámetros\n",
    "\n",
    "Para este caso usaremos el método **GridSearchCV** de Scikit-Learn, al cual podemos pasar un conjunto de valores para cada hyperparámetro. Este método construye y evalúa cada modelo con diferentes combinaciones para cada hyperparámetro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la librería a utilizar\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedemos ahora a definir nuestro \"grid\", el cual vamos a pasar a GridSearch, con los hyperparámetros con los que queremos probar cada modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regresión Logística:\n",
    "\n",
    "Para este modelo se decidió ajustar los hyperparámetros de **'penalty'**, con el cual podemos definir el tipo de normalización que queremos implementar sobre los datos del modelo; la idea de la normalización **l1** es que la sumatoria de los valores absolutos sea unitaria, mientras que la normalización **'l2'** busca que la sumatoria de los cuadrados sea unitaria. **'C'**, que es el inverso de la fuerza de regularización (la cual nos ayuda a evitar el sobre-ajuste en nuestros modelos); es decir, entre menor sea el valor de C, mayor es la regularización aplicada al modelo. **'Solver'** con el cual definimos el algotitmo de optimización a implementar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se tiene un Score de 0.939646 utilizando {'C': 50, 'penalty': 'l1', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "# Definiendo el grid para la regresión logística\n",
    "\n",
    "lr = LogisticRegression()\n",
    "penalty=['l1', 'l2']\n",
    "C=[500, 100, 50, 1, 0.1, 0.01]\n",
    "solver= ['liblinear', 'saga']\n",
    "\n",
    "## Definimos el diccionario con los parámetros\n",
    "param_grid = dict(penalty=penalty, C=C, solver=solver)\n",
    "\n",
    "## Ingresamos los parámetros\n",
    "grid = GridSearchCV(estimator=lr,\n",
    "                    param_grid=param_grid,\n",
    "                    scoring='roc_auc',\n",
    "                    n_jobs=-1)\n",
    "\n",
    "## Ajustamos a los datos de entrenamiento\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Se tiene un Score de %f utilizando %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos la métrica Score de GridSearchCV sobre el conjunto de entrenamiento, la cual nos indica el mayor rendimiento alcanzado sobre el conjunto de entrenamiento, además de que imprimimos la combinación de hyperparámetros requerida para alcanzar dicho rendimiento. Para este modelo tenemos que alzanza su óptimo con C=50, Penalty = 'l1' y solver='liblinear'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bosque Aleatorio:\n",
    "\n",
    "En este caso se decidió variar los hyperparámetros **'n_estimator'**, con el cuál definimos el número de árboles en el modelo, y **'max_features'** con el cual definimos el número de caracteríticas a considerar cada vez que se vaya a hacer una partición en el árbol. Es decir, cada que se vaya a hacer una partición, se seleccionan N características aleatorias y se selecciona cuál es la mejor para dicho nodo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.944403 using {'max_features': 26, 'n_estimators': 90}\n",
      "Execution time: 473.596732378006 ms\n"
     ]
    }
   ],
   "source": [
    "# Definiendo el grid para bosque aleatorio\n",
    "\n",
    "rf=RandomForestClassifier()\n",
    "n_estimators=list(range(10,100,20))\n",
    "max_features=list(range(6,32,5))\n",
    "\n",
    "## Definimos el diccionario con los parámetros\n",
    "param_grid = dict(n_estimators=n_estimators, max_features=max_features)\n",
    "\n",
    "## Ingresamos los parámetros\n",
    "grid = GridSearchCV(estimator=rf,\n",
    "                    param_grid=param_grid,\n",
    "                    scoring='roc_auc',\n",
    "                    n_jobs=-1)\n",
    "\n",
    "## Ajustamos a los datos de entrenamiento\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Se tiene un Score de %f utilizando %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde obtenemos el mayor rendimiento sobre el conjunto de entrenamiento al seleccionar un máximo de 26 características para cada partición y 90 árboles en el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Descenso de Gradiente Estocástico: \n",
    "\n",
    "En este caso ajustamos los hyperparámetros **'penalty'**, definido anteriormente, y **'loss'**, con el cual definimso la función de pérdida o costo a utilizar: *hinge* es la función por defecto, y es utilizada para clasificación del margen-máximo entre los datos de las diferentes clases, mientras que *squared_hinge* cumple el mismo papel que la primera, pero con una función cuadrática. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se tiene un Score de 0.917105 utilizando {'loss': 'squared_hinge', 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "# Definiendo el grid para SGD\n",
    "\n",
    "sgd = linear_model.SGDClassifier()\n",
    "penalty = ['l1', 'l2']\n",
    "loss = ['hinge', 'squared_hinge']\n",
    "\n",
    "## Definimos el diccionario con los parámetros\n",
    "param_grid = dict(penalty=penalty, loss=loss)\n",
    "\n",
    "## Ingresamos los parámetros\n",
    "grid = GridSearchCV(estimator=sgd,\n",
    "                    param_grid=param_grid,\n",
    "                    scoring='roc_auc',\n",
    "                    n_jobs=-1)\n",
    "\n",
    "## Ajustamos a los datos de entrenamiento\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Se tiene un Score de %f utilizando %s\" % (grid_result.best_score_, grid_result.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acuerdo con los resultados, tenemos un mayor rendimiento al definir loss = 'squared_hinge'´y penalty = 'l2'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probamos cada modelo con los Hyperparámetros seleccionados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apliquemos validación cruzadasobre los datos de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se obtiene un rendimiento de 88.4% con Regresión Logística\n",
      "Se obtiene un rendimiento de 89.6% con Random Forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se obtiene un redimiento de 84.6% con SGD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Se ajusta el modelo de Regresión Logística (lr) a los datos y se mide el rendimiento promedio\n",
    "lr = LogisticRegression(C=50, penalty='l1', solver='liblinear') \n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "score = cross_val_score(lr, X_train, y_train, cv=5) \n",
    "print(\"Se obtiene un rendimiento de {0:.1%} con Regresión Logística\".format(score.mean()))\n",
    "\n",
    "\n",
    "## Entrenamos el modelo de Bosque Aleatorio y medimos el rendimiento promedio\n",
    "rf = RandomForestClassifier(max_features=26, n_estimators=90)  \n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "score = cross_val_score(rf, X_train, y_train, cv=5)\n",
    "print(\"Se obtiene un rendimiento de {0:.1%} con Random Forest\".format(score.mean()))\n",
    "\n",
    "\n",
    "## Entrenamos el modelo de Descenso del Gradiente Estocástico y medimos el rendimiento promedio\n",
    "sgd = linear_model.SGDClassifier(loss='squared_hinge', penalty='l2') \n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "score = cross_val_score(sgd, X_train, y_train, cv=5)\n",
    "print(\"Se obtiene un redimiento de {0:.1%} con SGD\".format(score.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se obtivo una mejora en rendimiento del 3.8% para el modelo de Regresión Logrística y 2.2% para el modelo de Bosque Aleatorio, mientras que el rendimiento del Descenso de Gradiente Estocástico fue ligeramente más bajo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora procedemos a medir las métricas de rendimiento nuevamente y comparamos con las obtenidas para los modelos base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se obtiene una precisión de 86.5% con Regresión Logística\n",
      "Se obtiene una precisión de 94.9% con Random Forest\n",
      "Se obtiene una precisión de 79.8% con SGD\n",
      "\n",
      "Se obtiene un recall de 82.9% con Regresión Logística\n",
      "Se obtiene un recall de 75.8% con Random Forest\n",
      "Se obtiene un recall de 81.4% con SGD\n",
      "\n",
      "Se obtiene un F1-score de 84.7% con Regresión Logística\n",
      "Se obtiene un F1-score de 84.3% con Random Forest\n",
      "Se obtiene un F1-score de 80.6% con SGD\n"
     ]
    }
   ],
   "source": [
    "## Evaluación de los clasificadores\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "##Precision\n",
    "lr_score = precision_score(y_test, lr.predict(X_test))\n",
    "rf_score = precision_score(y_test, rf.predict(X_test))\n",
    "sgd_score = precision_score(y_test, sgd.predict(X_test))\n",
    "\n",
    "print(\"Se obtiene una precisión de {0:.1%} con Regresión Logística\".format(lr_score))\n",
    "print(\"Se obtiene una precisión de {0:.1%} con Random Forest\".format(rf_score))\n",
    "print(\"Se obtiene una precisión de {0:.1%} con SGD\\n\".format(sgd_score))\n",
    "\n",
    "\n",
    "##Recall\n",
    "lr_score = recall_score(y_test, lr.predict(X_test))\n",
    "rf_score = recall_score(y_test, rf.predict(X_test))\n",
    "sgd_score = recall_score(y_test, sgd.predict(X_test))\n",
    "\n",
    "print(\"Se obtiene un recall de {0:.1%} con Regresión Logística\".format(lr_score))\n",
    "print(\"Se obtiene un recall de {0:.1%} con Random Forest\".format(rf_score))\n",
    "print(\"Se obtiene un recall de {0:.1%} con SGD\\n\".format(sgd_score))\n",
    "\n",
    "\n",
    "##F1_score\n",
    "lr_score = f1_score(y_test, lr.predict(X_test))\n",
    "rf_score = f1_score(y_test, rf.predict(X_test))\n",
    "sgd_score = f1_score(y_test, sgd.predict(X_test))\n",
    "\n",
    "print(\"Se obtiene un F1-score de {0:.1%} con Regresión Logística\".format(lr_score))\n",
    "print(\"Se obtiene un F1-score de {0:.1%} con Random Forest\".format(rf_score))\n",
    "print(\"Se obtiene un F1-score de {0:.1%} con SGD\".format(sgd_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión Regresión Lineal:\n",
      " [[2769  224]\n",
      " [ 296 1436]] \n",
      "\n",
      "Matriz de confusión Random Forest:\n",
      " [[2923   70]\n",
      " [ 419 1313]] \n",
      "\n",
      "Matriz de confusión SGD:\n",
      " [[2635  358]\n",
      " [ 322 1410]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Confusion matrix\n",
    "lr_cf = confusion_matrix(y_test, lr.predict(X_test))\n",
    "print(\"Matriz de confusión Regresión Lineal:\\n\", lr_cf, \"\\n\")\n",
    "\n",
    "rf_cf = confusion_matrix(y_test, rf.predict(X_test))\n",
    "print(\"Matriz de confusión Random Forest:\\n\", rf_cf, \"\\n\")\n",
    "\n",
    "sgd_cf = confusion_matrix(y_test, sgd.predict(X_test))\n",
    "print(\"Matriz de confusión SGD:\\n\", sgd_cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observaciones:\n",
    "- El modelo de Regresión Logística presenta una disminución del 6% en su rendimiento de acuerdo con la métrica *precision*, sin embargo muestra una mejora significativa para la métrica de *recall* y un mejor rendimiento de acuerdo con el *f1-score*. Esto quiere decir que con la combinación de hyperparámetros seleccionada logramos que el modelo realice un mejor trabajo para clasificar los *péptidos antimicrobianos* en su categoría correcta, sacrificando un poco su desempeño a la hora de no clasificar *péptidos no-antimicrobianor* como antimicrobianos (mirar matriz de confusión).\n",
    "- El modelo de Bosque Aleatorio obtuvo una mejora del 2% al comparar el modelo Base y el modelo con los hyperparámetros ajustados para las métrica *precision*, *recall* y *f1-score*, que se puede constatar si miramos la matriz de confusión donde se nos muestra, con los hyperparámetros seleccionados, el modelo realiza mejor el proceso de clasificación.  \n",
    "- Al igual que el modelo de Regresión Logística, con el modelo de Descenso de Gradiente Estocástico obtenemos una mejora a la hora de clasificar los *péptidos antimicrobianos* en su categoría correcta de acuerdo con el *recall*, sin embargo, el desempeño del modelo para clasificar los *péptidos no-antimicrobianos* dentro de su categoría se ve disminuído, de acuerdo con el *precision* y la *matriz de confusión*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Aplicando selección de características\n",
    "\n",
    "A continuación se aplica un método de selección de carasterísticas sobre los datos y luego se miden las métricas para cada modelo con los hyperparámetros determinados en el paso anterior. Esto con el objetivo de identificar si seleccionar las características principales significa una mejora en la capacidad de clasificación de los modelos. \n",
    "\n",
    "Para ello se utiliza un Eliminador de Características Recursivo, *RFE*, por sus siglas en inglés, al cual se le ingresa el número de características máximo que queremos seleccionar. \n",
    "\n",
    "\n",
    "De manera aleatoria se ha decidido probar cada modelo al seleccionar 1000, 500 y 100 características. En cada caso se mide el *Accuracy* con objetivo de comparar el desempeño de los modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RFE con Regresión Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionando 1000 características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 1758 features.\n",
      "Fitting estimator with 1658 features.\n",
      "Fitting estimator with 1558 features.\n",
      "Fitting estimator with 1458 features.\n",
      "Fitting estimator with 1358 features.\n",
      "Fitting estimator with 1258 features.\n",
      "Fitting estimator with 1158 features.\n",
      "Fitting estimator with 1058 features.\n",
      "88.9% accuracy on test set.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# se crea modelo con Regresión Logística y se seleccionan 1000 características, eliminando 100 a cada paso\n",
    "lr = LogisticRegression(C=50, penalty='l1', solver='liblinear') \n",
    "rfe_lr1 = RFE(estimator=lr, n_features_to_select=1000, step = 100, verbose=1)\n",
    "\n",
    "#Ajustando modelo a datos\n",
    "rfe_lr1.fit(X_train, y_train)\n",
    "\n",
    "#calculamos el accuracy\n",
    "\n",
    "acc = accuracy_score(y_test, rfe_lr1.predict(X_test))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionando 500 características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 1758 features.\n",
      "Fitting estimator with 1558 features.\n",
      "Fitting estimator with 1358 features.\n",
      "Fitting estimator with 1158 features.\n",
      "Fitting estimator with 958 features.\n",
      "Fitting estimator with 758 features.\n",
      "Fitting estimator with 558 features.\n",
      "89.3% accuracy on test set.\n"
     ]
    }
   ],
   "source": [
    "# se crea modelo con Regresión Logística y se seleccionan 500 características, eliminando 200 a cada paso. \n",
    "rfe_lr2 = RFE(estimator=lr, n_features_to_select=500, step = 200, verbose=1)\n",
    "\n",
    "#Ajustando modelo a datos\n",
    "rfe_lr2.fit(X_train, y_train)\n",
    "\n",
    "#calculamos el accuracy\n",
    "\n",
    "acc = accuracy_score(y_test, rfe_lr2.predict(X_test))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionando 100 características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 1758 features.\n",
      "Fitting estimator with 1458 features.\n",
      "Fitting estimator with 1158 features.\n",
      "Fitting estimator with 858 features.\n",
      "Fitting estimator with 558 features.\n",
      "Fitting estimator with 258 features.\n",
      "87.8% accuracy on test set.\n"
     ]
    }
   ],
   "source": [
    "# se crea modelo con Regresión Logística y se seleccionan 1000 características\n",
    "rfe_lr3 = RFE(estimator=lr, n_features_to_select=100, step = 300, verbose=1)\n",
    "\n",
    "#Ajustando modelo a datos\n",
    "rfe_lr3.fit(X_train, y_train)\n",
    "\n",
    "#calculamos el accuracy\n",
    "\n",
    "acc = accuracy_score(y_test, rfe_lr3.predict(X_test))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se presenta un cambio muy significativo en la exactitud del modelo al seleccionar 1000, 500 o 100 características, siendo el mayor accuracy (89.3%) el alcanzado con 500 características seleccionadas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RFE con modelo de Bosque Aleatorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionando 1000 características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 1758 features.\n",
      "Fitting estimator with 1658 features.\n",
      "Fitting estimator with 1558 features.\n",
      "Fitting estimator with 1458 features.\n",
      "Fitting estimator with 1358 features.\n",
      "Fitting estimator with 1258 features.\n",
      "Fitting estimator with 1158 features.\n",
      "Fitting estimator with 1058 features.\n",
      "89.9% accuracy on test set.\n"
     ]
    }
   ],
   "source": [
    "# se crea modelo con Random Forest y se seleccionan 1000 características, eliminando 100 a cada paso\n",
    "rf = RandomForestClassifier(max_features=26, n_estimators=90) \n",
    "rfe_rf1 = RFE(estimator=rf, n_features_to_select=1000, step = 100, verbose=1)\n",
    "\n",
    "#Ajustando modelo a datos\n",
    "rfe_rf1.fit(X_train, y_train)\n",
    "\n",
    "#calculamos el accuracy\n",
    "\n",
    "acc = accuracy_score(y_test, rfe_rf1.predict(X_test))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionando 500 características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 1758 features.\n",
      "Fitting estimator with 1558 features.\n",
      "Fitting estimator with 1358 features.\n",
      "Fitting estimator with 1158 features.\n",
      "Fitting estimator with 958 features.\n",
      "Fitting estimator with 758 features.\n",
      "Fitting estimator with 558 features.\n",
      "90.0% accuracy on test set.\n"
     ]
    }
   ],
   "source": [
    "# se crea modelo con Random Forest y se seleccionan 500 características, eliminando 200 a cada paso\n",
    "rfe_rf2 = RFE(estimator=rf, n_features_to_select=500, step = 200, verbose=1)\n",
    "\n",
    "#Ajustando modelo a datos\n",
    "rfe_rf2.fit(X_train, y_train)\n",
    "\n",
    "#calculamos el accuracy\n",
    "\n",
    "acc = accuracy_score(y_test, rfe_rf2.predict(X_test))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionando 100 características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 1758 features.\n",
      "Fitting estimator with 1458 features.\n",
      "Fitting estimator with 1158 features.\n",
      "Fitting estimator with 858 features.\n",
      "Fitting estimator with 558 features.\n",
      "Fitting estimator with 258 features.\n",
      "90.3% accuracy on test set.\n"
     ]
    }
   ],
   "source": [
    "# se crea modelo con Random Forest y se seleccionan 100 características, eliminando 300 a cada paso\n",
    "rfe_rf3 = RFE(estimator=rf, n_features_to_select=100, step = 300, verbose=1)\n",
    "\n",
    "#Ajustando modelo a datos\n",
    "rfe_rf3.fit(X_train, y_train)\n",
    "\n",
    "#calculamos el accuracy\n",
    "\n",
    "acc = accuracy_score(y_test, rfe_rf3.predict(X_test))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mayor exactitud del modelo de Bosque Aleatorio se obtiene al seleccionar 100 carafcterísticas del total, arrojando un accurary=90.3%, no muy superior al obtenido con 1000 y 500 características seleccionadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RFE con Descenso de Gradiente Estocástico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionando 1000 características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 1758 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 1658 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 1558 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 1458 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 1358 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 1258 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 1158 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 1058 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.1% accuracy on test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# se crea modelo con SGD y se seleccionan 1000 características, eliminando 100 a cada paso\n",
    "sgd = linear_model.SGDClassifier(loss='squared_hinge', penalty='l2') \n",
    "rfe_sgd1 = RFE(estimator=sgd, n_features_to_select=1000, step = 100, verbose=1)\n",
    "\n",
    "#Ajustando modelo a datos\n",
    "rfe_sgd1.fit(X_train, y_train)\n",
    "\n",
    "#calculamos el accuracy\n",
    "\n",
    "acc = accuracy_score(y_test, rfe_sgd1.predict(X_test))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionando 500 características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 1758 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 1558 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 1358 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 1158 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 958 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 758 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 558 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.3% accuracy on test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# se crea modelo con SGD y se seleccionan 500 características, eliminando 200 a cada paso\n",
    "rfe_sgd2 = RFE(estimator=sgd, n_features_to_select=500, step = 200, verbose=1)\n",
    "\n",
    "#Ajustando modelo a datos\n",
    "rfe_sgd2.fit(X_train, y_train)\n",
    "\n",
    "#calculamos el accuracy\n",
    "\n",
    "acc = accuracy_score(y_test, rfe_sgd2.predict(X_test))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionando 100 características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 1758 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 1458 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 1158 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 858 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 558 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 258 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.8% accuracy on test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# se crea modelo con SGD y se seleccionan 100 características, eliminando 300 a cada paso\n",
    "rfe_sgd3 = RFE(estimator=sgd, n_features_to_select=100, step = 300, verbose=1)\n",
    "\n",
    "#Ajustando modelo a datos\n",
    "rfe_sgd3.fit(X_train, y_train)\n",
    "\n",
    "#calculamos el accuracy\n",
    "\n",
    "acc = accuracy_score(y_test, rfe_sgd3.predict(X_test))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se alcanzó un *accuracy* similar al seleccionar 1000 y 500 características, mientras que el mismo disminuyó al seleccionar solo 100 características del total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomamos los casos en que se tuvo un mejor accuracy en cada modelo para medir las métricas de *precision*, *recall*, *f1-score* y *confusion matrix*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se obtiene una precisión de 87.7% con Regresión Logística\n",
      "Se obtiene una precisión de 91.2% con Random Forest\n",
      "Se obtiene una precisión de 80.1% con SGD\n",
      "\n",
      "Se obtiene un recall de 82.3% con Regresión Logística\n",
      "Se obtiene un recall de 81.4% con Random Forest\n",
      "Se obtiene un recall de 83.1% con SGD\n",
      "\n",
      "Se obtiene un F1-score de 84.9% con Regresión Logística\n",
      "Se obtiene un F1-score de 86.0% con Random Forest\n",
      "Se obtiene un F1-score de 81.6% con SGD\n"
     ]
    }
   ],
   "source": [
    "## Evaluación de los clasificadores\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "##Precision\n",
    "lr_score = precision_score(y_test, rfe_lr2.predict(X_test))\n",
    "rf_score = precision_score(y_test, rfe_rf3.predict(X_test))\n",
    "sgd_score = precision_score(y_test, rfe_sgd2.predict(X_test))\n",
    "\n",
    "print(\"Se obtiene una precisión de {0:.1%} con Regresión Logística\".format(lr_score))\n",
    "print(\"Se obtiene una precisión de {0:.1%} con Random Forest\".format(rf_score))\n",
    "print(\"Se obtiene una precisión de {0:.1%} con SGD\\n\".format(sgd_score))\n",
    "\n",
    "\n",
    "##Recall\n",
    "lr_score = recall_score(y_test, rfe_lr2.predict(X_test))\n",
    "rf_score = recall_score(y_test, rfe_rf3.predict(X_test))\n",
    "sgd_score = recall_score(y_test, rfe_sgd2.predict(X_test))\n",
    "\n",
    "print(\"Se obtiene un recall de {0:.1%} con Regresión Logística\".format(lr_score))\n",
    "print(\"Se obtiene un recall de {0:.1%} con Random Forest\".format(rf_score))\n",
    "print(\"Se obtiene un recall de {0:.1%} con SGD\\n\".format(sgd_score))\n",
    "\n",
    "\n",
    "##F1_score\n",
    "lr_score = f1_score(y_test, rfe_lr2.predict(X_test))\n",
    "rf_score = f1_score(y_test, rfe_rf3.predict(X_test))\n",
    "sgd_score = f1_score(y_test, rfe_sgd2.predict(X_test))\n",
    "\n",
    "print(\"Se obtiene un F1-score de {0:.1%} con Regresión Logística\".format(lr_score))\n",
    "print(\"Se obtiene un F1-score de {0:.1%} con Random Forest\".format(rf_score))\n",
    "print(\"Se obtiene un F1-score de {0:.1%} con SGD\".format(sgd_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión Regresión Lineal:\n",
      " [[2766  227]\n",
      " [ 298 1434]] \n",
      "\n",
      "Matriz de confusión Random Forest:\n",
      " [[2857  136]\n",
      " [ 322 1410]] \n",
      "\n",
      "Matriz de consufión SGD:\n",
      " [[2627  366]\n",
      " [ 292 1440]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Confusion matrix\n",
    "lr_cf = confusion_matrix(y_test, rfe_lr1.predict(X_test))\n",
    "print(\"Matriz de confusión Regresión Lineal:\\n\", lr_cf, \"\\n\")\n",
    "\n",
    "rf_cf = confusion_matrix(y_test, rfe_rf3.predict(X_test))\n",
    "print(\"Matriz de confusión Random Forest:\\n\", rf_cf, \"\\n\")\n",
    "\n",
    "sgd_cf = confusion_matrix(y_test, rfe_sgd1.predict(X_test))\n",
    "print(\"Matriz de consufión SGD:\\n\", sgd_cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observaciones:\n",
    "- El modelo de Regresión Logística, con 500 características seleccionadas, no mostró una variación significativa para ninguna de las 4 métricas. Lo que significa que podemos clasificar los péptidos con solo 500 de los 1758 descriptores originales. \n",
    "- Para el modelo de Bosque Aleatorio se seleccionaron 100 características, obteniéndose una disminución en la métrica de *precision* y una mejora en las métricas de *recall* y *f1-score*. Indicando que la capacidad del modelo para clasificar los *péptidos antimicrobianos* mejoró después de aplicar selección de características, como se puede observar en la matríz de confusión.\n",
    "- El modelo de Descenso de Gradiente Estocástico presenta una ligera mejora en las cuatro métricas después de seleccionar solo 500 de los descriptores para la clasificación. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando extracción de características\n",
    "\n",
    "A continuación se aplica un método de extracción de carasterísticas sobre los datos y luego se miden las métricas para cada modelo. Como en el caso anterior, se evalúa cada modelo extrayendo 1000, 500 y 100 componentes. \n",
    "\n",
    "\n",
    "Para este caso se construirá un **pipeline**, donde se utilizará *Análisis de Componentes principales (PCA)* para la extracción de las características a utilizar, combinado con cada uno de los modelos de clasificación.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se importan las librerías a utilizar\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracción de características + Regresión Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrayendo 1000 componentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.2% test set accuracy\n"
     ]
    }
   ],
   "source": [
    "pipe_lr1 = Pipeline([\n",
    "        ('reducer', PCA(n_components=1000)),\n",
    "        ('classifier', LogisticRegression(C=50, penalty='l1', solver='liblinear'))]) \n",
    "\n",
    "pipe_lr1.fit(X_train, y_train)\n",
    "\n",
    "# Score the accuracy on the test set\n",
    "accuracy = pipe_lr1.score(X_test, y_test)\n",
    "\n",
    "# Prints the model accuracy\n",
    "print('{0:.1%} test set accuracy'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrayendo 500 componentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.9% test set accuracy\n"
     ]
    }
   ],
   "source": [
    "pipe_lr2 = Pipeline([\n",
    "        ('reducer', PCA(n_components=500)),\n",
    "        ('classifier', LogisticRegression(C=50, penalty='l1', solver='liblinear'))])\n",
    "\n",
    "pipe_lr2.fit(X_train, y_train)\n",
    "\n",
    "# Score the accuracy on the test set\n",
    "accuracy = pipe_lr2.score(X_test, y_test)\n",
    "\n",
    "# Prints the model accuracy\n",
    "print('{0:.1%} test set accuracy'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrayendo 100 componentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.0% test set accuracy\n"
     ]
    }
   ],
   "source": [
    "pipe_lr3 = Pipeline([\n",
    "        ('reducer', PCA(n_components=100)),\n",
    "        ('classifier', LogisticRegression(C=50, penalty='l1', solver='liblinear'))])\n",
    "\n",
    "pipe_lr3.fit(X_train, y_train)\n",
    "\n",
    "# Score the accuracy on the test set\n",
    "accuracy = pipe_lr3.score(X_test, y_test)\n",
    "\n",
    "# Prints the model accuracy\n",
    "print('{0:.1%} test set accuracy'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtiene una mejor capacidad de clasificación (88.2%) al extraer 1000 características del conjunto original. Además, la exactitud del modelo de Regresión Logística disminuyó a medida que disminuimos el número de características extraídas.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracción de características + Bosque Aleatorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrayendo 1000 componentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.7% test set accuracy\n"
     ]
    }
   ],
   "source": [
    "pipe_rf1 = Pipeline([\n",
    "        ('reducer', PCA(n_components=1000)),\n",
    "        ('classifier', RandomForestClassifier(max_features=26, n_estimators=90))])\n",
    "\n",
    "pipe_rf1.fit(X_train, y_train)\n",
    "\n",
    "# Score the accuracy on the test set\n",
    "accuracy = pipe_rf1.score(X_test, y_test)\n",
    "\n",
    "# Prints the model accuracy\n",
    "print('{0:.1%} test set accuracy'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrayendo 500 componentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.4% test set accuracy\n"
     ]
    }
   ],
   "source": [
    "pipe_rf2 = Pipeline([\n",
    "        ('reducer', PCA(n_components=500)),\n",
    "        ('classifier', RandomForestClassifier(max_features=26, n_estimators=90))])\n",
    "\n",
    "pipe_rf2.fit(X_train, y_train)\n",
    "\n",
    "# Score the accuracy on the test set\n",
    "accuracy = pipe_rf2.score(X_test, y_test)\n",
    "\n",
    "# Prints the model accuracy\n",
    "print('{0:.1%} test set accuracy'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrayendo 100 componentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.4% test set accuracy\n"
     ]
    }
   ],
   "source": [
    "pipe_rf3 = Pipeline([\n",
    "        ('reducer', PCA(n_components=100)),\n",
    "        ('classifier', RandomForestClassifier(max_features=26, n_estimators=90))])\n",
    "\n",
    "pipe_rf3.fit(X_train, y_train)\n",
    "\n",
    "# Score the accuracy on the test set\n",
    "accuracy = pipe_rf3.score(X_test, y_test)\n",
    "\n",
    "# Prints the model accuracy\n",
    "print('{0:.1%} test set accuracy'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el modelo de Bosque aleatorio se observa un comportamiento contrario al de Regresión Logística, puesto que en este caso la capacidad de clasificación del modelo aumenta a medida que disminuye el número de características extraídas, obteniéndose la mayor exactitud (86.4%) al extraer 100 características."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracción de características + Descenso de Gradiente Estocástico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrayendo 1000 componentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.5% test set accuracy\n"
     ]
    }
   ],
   "source": [
    "pipe_sgd1 = Pipeline([\n",
    "        ('reducer', PCA(n_components=1000)),\n",
    "        ('classifier', linear_model.SGDClassifier(loss='squared_hinge', penalty='l2'))])\n",
    "\n",
    "pipe_sgd1.fit(X_train, y_train)\n",
    "\n",
    "# Score the accuracy on the test set\n",
    "accuracy = pipe_sgd1.score(X_test, y_test)\n",
    "\n",
    "# Prints the model accuracy\n",
    "print('{0:.1%} test set accuracy'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrayendo 500 componentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.3% test set accuracy\n"
     ]
    }
   ],
   "source": [
    "pipe_sgd2 = Pipeline([\n",
    "        ('reducer', PCA(n_components=500)),\n",
    "        ('classifier', linear_model.SGDClassifier(loss='squared_hinge', penalty='l2'))])\n",
    "\n",
    "pipe_sgd2.fit(X_train, y_train)\n",
    "\n",
    "# Score the accuracy on the test set\n",
    "accuracy = pipe_sgd2.score(X_test, y_test)\n",
    "\n",
    "# Prints the model accuracy\n",
    "print('{0:.1%} test set accuracy'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrayendo 100 componentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.5% test set accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "pipe_sgd3 = Pipeline([\n",
    "        ('reducer', PCA(n_components=100)),\n",
    "        ('classifier', linear_model.SGDClassifier(loss='squared_hinge', penalty='l2'))])\n",
    "\n",
    "pipe_sgd3.fit(X_train, y_train)\n",
    "\n",
    "# Score the accuracy on the test set\n",
    "accuracy = pipe_sgd3.score(X_test, y_test)\n",
    "\n",
    "# Prints the model accuracy\n",
    "print('{0:.1%} test set accuracy'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este modelo se observa una mejor capacidad de clasificación con 1000 componentes, mientras que la exactitud del mismo disminuye al extraer 500 y 100 componentes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A contiuación se miden las métricas para cada modelo tomando el número de componentes en que cada uno mostró un mejor desempeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se obtiene una precisión de 85.7% con Regresión Logística\n",
      "Se obtiene una precisión de 90.8% con Random Forest\n",
      "Se obtiene una precisión de 79.6% con SGD\n",
      "\n",
      "Se obtiene un recall de 81.3% con Regresión Logística\n",
      "Se obtiene un recall de 70.0% con Random Forest\n",
      "Se obtiene un recall de 81.4% con SGD\n",
      "\n",
      "Se obtiene un F1-score de 83.5% con Regresión Logística\n",
      "Se obtiene un F1-score de 79.1% con Random Forest\n",
      "Se obtiene un F1-score de 80.5% con SGD\n"
     ]
    }
   ],
   "source": [
    "## Evaluación de los clasificadores\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "##Precision\n",
    "lr_score = precision_score(y_test, pipe_lr1.predict(X_test))\n",
    "rf_score = precision_score(y_test, pipe_rf3.predict(X_test))\n",
    "sgd_score = precision_score(y_test, pipe_sgd1.predict(X_test))\n",
    "\n",
    "print(\"Se obtiene una precisión de {0:.1%} con Regresión Logística\".format(lr_score))\n",
    "print(\"Se obtiene una precisión de {0:.1%} con Random Forest\".format(rf_score))\n",
    "print(\"Se obtiene una precisión de {0:.1%} con SGD\\n\".format(sgd_score))\n",
    "\n",
    "\n",
    "##Recall\n",
    "lr_score = recall_score(y_test, pipe_lr1.predict(X_test))\n",
    "rf_score = recall_score(y_test, pipe_rf3.predict(X_test))\n",
    "sgd_score = recall_score(y_test,pipe_sgd1.predict(X_test))\n",
    "\n",
    "print(\"Se obtiene un recall de {0:.1%} con Regresión Logística\".format(lr_score))\n",
    "print(\"Se obtiene un recall de {0:.1%} con Random Forest\".format(rf_score))\n",
    "print(\"Se obtiene un recall de {0:.1%} con SGD\\n\".format(sgd_score))\n",
    "\n",
    "\n",
    "##F1_score\n",
    "lr_score = f1_score(y_test, pipe_lr1.predict(X_test))\n",
    "rf_score = f1_score(y_test, pipe_rf3.predict(X_test))\n",
    "sgd_score = f1_score(y_test, pipe_sgd1.predict(X_test))\n",
    "\n",
    "print(\"Se obtiene un F1-score de {0:.1%} con Regresión Logística\".format(lr_score))\n",
    "print(\"Se obtiene un F1-score de {0:.1%} con Random Forest\".format(rf_score))\n",
    "print(\"Se obtiene un F1-score de {0:.1%} con SGD\".format(sgd_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión Regresión Lineal:\n",
      " [[2759  234]\n",
      " [ 324 1408]] \n",
      "\n",
      "Matriz de confusión Random Forest:\n",
      " [[2870  123]\n",
      " [ 519 1213]] \n",
      "\n",
      "Matriz de consufión SGD:\n",
      " [[2632  361]\n",
      " [ 323 1409]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Confusion matrix\n",
    "lr_cf = confusion_matrix(y_test, pipe_lr1.predict(X_test))\n",
    "print(\"Matriz de confusión Regresión Lineal:\\n\", lr_cf, \"\\n\")\n",
    "\n",
    "rf_cf = confusion_matrix(y_test, pipe_rf3.predict(X_test))\n",
    "print(\"Matriz de confusión Random Forest:\\n\", rf_cf, \"\\n\")\n",
    "\n",
    "sgd_cf = confusion_matrix(y_test, pipe_sgd1.predict(X_test))\n",
    "print(\"Matriz de consufión SGD:\\n\", sgd_cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observaciones:\n",
    "- Con el proceso de extracción de características, se observa una disminución en la capacidad de clasificación de lo *péptidos antimicrobianos* de los tres modelos, de acuerdo con los resultados obtenidos para las cuatro métricas. \n",
    "- Con estos resultados concluímos que el proceso de extracción de características no aporta un mejora a la hora de clasificar los péptidos, por lo que puede ser descartado de la solución. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
